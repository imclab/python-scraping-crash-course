{
 "metadata": {
  "name": "Scrape Link\u00f6ping landlords"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": "A crash course in scraping websites and indexing in Elasticsearch"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "*\"I (\u2026) am rarely happier than when spending an entire day programming my computer to perform automatically a task that would otherwise take me a good ten seconds to do by hand.\" \u2014 Douglas Adams, Last Chance to See*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "My girlfriend and I plan to move together. We have been in the queue for too short to get an apartment through the biggest actor, St\u00e5ng\u00e5staden. Luckily, the city website has contact information to a good number of local, smaller landlords without queueing systems. Unfortunately, the website is a pain to work with.\n\nWe are interested in two or three areas of town, close to the center. The website lets us list landlords active in one area of town at a time. Further, the landlord listings do not include detailed information; you have to scan detail pages and look for an email address, phone number, or whatever in semi-structured HTML to find out how to contact each of them. This all generates tedious cross-referencing work, just to find the landlords we should contact.\n\nThis article describes how to scrape the Link\u00f6ping website for landlords, index them in [Elasticsearch](http://www.elasticsearch.org/), and present only the relevant ones along with a list of email addresses suitable for copypasting to an email client."
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": "Scrape"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "We use [requests](http://docs.python-requests.org/en/latest/) for HTTP and [pyquery](http://pythonhosted.org/pyquery/) to work with HTML. Figuring out how to scrape [linkoping.se](http://linkoping.se/) is not as straighforward as one would hope. I wonder if [EPiSERVER](http://www.episerver.se/) is to blame."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import re\n\nfrom pyquery import PyQuery\nimport requests\n\nLANDLORDS_URL = \"http://www.linkoping.se/sv/Bygga-bo/Hitta-bostad/Hyresvardar-i-Linkoping/\"\nPLACE_SELECT_NAME = \"ctl00$FullContentRegion$FullRightContentArea$MainAndSideBodyRegion$SelectionListPage2$DropDownList1\"\nSHOW_SELECT_NAME = \"ctl00$FullContentRegion$FullRightContentArea$MainAndSideBodyRegion$SelectionListPage2$btnShow\"\nSHOW_SELECT_VALUE = \"Visa\"\n\n\ndef get_place_options():\n    r = requests.get(LANDLORDS_URL)\n    r.raise_for_status()\n    pq = PyQuery(r.text)\n    place_options = pq(\"select[name='{}']\".format(PLACE_SELECT_NAME)).children()\n    \n    def mapper(i, el):\n        place = PyQuery(el)\n        return place.text(), place.attr(\"value\")\n    \n    return place_options.map(mapper)\n\n\ndef get_landlord_links(place_value):\n    r = requests.post(LANDLORDS_URL, data={\n        PLACE_SELECT_NAME: place_value, \n        SHOW_SELECT_NAME: SHOW_SELECT_VALUE,\n        \"__EVENTTARGET\": PLACE_SELECT_NAME})\n    r.raise_for_status()\n    pq = PyQuery(r.text)\n    pq.make_links_absolute(base_url=LANDLORDS_URL)\n    landlord_links = pq(\"table#PageListTable tr a.PageListItemHeading\")\n    \n    def mapper(i, el):\n        landlord_link = PyQuery(el)\n        return landlord_link.text(), landlord_link.attr(\"href\")\n    \n    return landlord_links.map(mapper)\n\n\ndef get_landlord_info(link):\n    r = requests.get(link)\n    r.raise_for_status()\n    pq = PyQuery(r.text)\n    pq.make_links_absolute(base_url=link)\n    \n    info = Info()\n    main = pq(\".mainBody > p\")\n    for content in main.contents():\n        is_text = isinstance(content, basestring)\n        if is_text:\n            info.current_text.append(content.strip(\" :\"))\n        elif content.text:\n            tag = content.tag\n            if tag == \"br\":\n                info.current_text.append(u\"\\n\")\n            elif tag == \"a\":\n                href = PyQuery(content).attr(\"href\")\n                info.current_text.append(href.replace(\"mailto:\", \"\"))\n            elif tag in [\"span\", \"strong\"]:\n                info.add()\n                info.current_key = content.text.strip(\" :\")\n            else:\n                raise Exception(\"unexpected tag: {}\".format(tag))\n    info.add()\n    info.parsed = info.parsed[1:]  # always the address and key = landlord name\n    info.parsed.append((\"text\", main.text()))\n    info.parsed.append((\"html\", main.html()))\n    \n    meta_description = pq(\"meta[name='EPi.Description']\").attr(\"content\")\n    if meta_description:\n        emails = re.findall(r\"(\\b[\\w.]+@+[\\w.]+.+[\\w.]\\b)\", meta_description)\n        if emails:\n            assert len(emails) == 1\n            info.parsed.append((\"meta_email\", emails[0]))\n            \n    return info.parsed\n    \n\nclass Info(object):\n    \n    def __init__(self):\n        self.current_key = \"\"\n        self.current_text = []\n        self.parsed = []\n    \n    def add(self):\n        if self.current_key and self.current_text:\n            fixed_key = \"_\" + self.current_key.strip().encode(\"ascii\", \"ignore\").lower() \\\n                            .replace(\" \", \"_\").replace(\"-\", \"\")\n            self.parsed.append((fixed_key, u\"\".join(self.current_text)))\n            self.current_key = \"\"\n            self.current_text = []",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Now we are ready to perform the actual scraping and put all information in *landlords*."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import collections\n\nlandlords = []\nplace_options = get_place_options()\n\nlandlord_places = collections.defaultdict(set)\nfor place_name, place_option in place_options[1:]:\n    for landlord, link in get_landlord_links(place_option):\n        landlord_places[landlord].add(place_name)\n        \nall_places = place_options[0][1]\nfor landlord, link in get_landlord_links(all_places):\n    info = {k: v for k, v in get_landlord_info(link)}\n    info[u\"name\"] = landlord\n    info[u\"link\"] = link\n    info[u\"areas\"] = list(landlord_places.get(landlord, []))\n    landlords.append(info)",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": "Index"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "We could do without a search engine but I want to get some experience of using [Elasticsearch](http://www.elasticsearch.org/). Let us start by preparing an index and a mapping containing all the fields we found when scraping. We say that all fields are strings and equally important. [Elasticsearch handles arrays](http://www.elasticsearch.org/guide/reference/mapping/array-type/) and detects that the *areas* field is an array of strings."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import itertools\nimport pyes\n\nes = pyes.ES(\"127.0.0.1:9200\")\nes.indices.delete_index_if_exists(\"landlords\")\nes.indices.create_index(\"landlords\")\n\nflatten = itertools.chain.from_iterable\nfields = set(flatten({f for f in landlord} for landlord in landlords))\nmapping_value = {\n    \"store\": \"yes\", \n    \"index\": \"analyzed\",\n    \"type\": \"string\",\n    \"boost\": 1.0,\n    \"term_vector\": \"with_positions_offsets\",\n    }\nmapping = {field.encode(\"utf-8\"): mapping_value for field in fields}\ntry:\n    es.delete_mapping(\"landlord\")\nexcept Exception:\n    pass\nes.put_mapping(\"landlord\", {'properties': mapping}, [\"landlords\"])\n\nfor landlord in landlords:\n    es.index(landlord, \"landlords\", \"landlord\")\n\nes.indices.refresh(\"landlords\")",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 3,
       "text": "{u'_shards': {u'failed': 0, u'successful': 5, u'total': 10}, u'ok': True}"
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": "Query"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Search for landlords active in the areas we are interested in."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import pyes.queryset\n\nmodel = pyes.queryset.generate_model(\"landlords\", \"landlord\")\nareas_of_interest = [\"vasastaden\", \"innerstaden\"]\nresults = list(model.objects.filter(areas__in=areas_of_interest))",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": "Present"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Dump results in something we can open in a web browser, including a list of email addresses as well as links to detail pages of landlords that we were unable to find email addresses to.\n\nThe output is not included here because it would be too easy for spambots to pick up email addresses (obviously the Link\u00f6ping website expose them though). Contact me if you are interested in it, or run the notebook yourself."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import os\n\nimport jinja2\n\nemails = []\nmissing_emails = []\nfor r in results:\n    email = r.get(\"meta_email\", r.get(\"_epost\", \"\"))\n    if email:\n        emails.append(email)\n    else:\n        missing_emails.append(r)\n\ntemplate = jinja2.Template(u\"\"\" \\\n<html>\n<head>\n<style type=\"text/css\">\n.bold { font-weight: bold; }\n</style>\n</head>\n<body>\n<h1>Hyresv\u00e4rdar</h1>\n{% for landlord in results %}\n<h2>{{landlord[\"name\"]}}</h2>\n<p>{{landlord[\"html\"]}}</p>\n{% endfor %}\n</dl>\n\n<h1>Epost</h1>\n<pre>\n{{\", \\n\".join(emails)}}\n</pre>\n\n<h1>Saknar epost</h1>\n<ul>\n{% for landlord in missing_emails %}\n<li>\n<a href=\"{{landlord[\"link\"]}}\">{{landlord[\"name\"]}}</a>\n</li>\n{% endfor %}\n</ul>\n</body>\n</html>\n\"\"\")\n\nwith open(os.path.expanduser(\"~/bo.html\"), \"wp\") as f:\n    f.write(template.render(results=results, emails=emails, missing_emails=missing_emails).encode(\"utf-8\"))",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": "Required packages and software"
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": "$ cat requirements.txt\nrequests==1.2.0\npyquery==1.2.4\npyes==0.20.0"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Elasticsearch should be running on localhost. I did"
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": "$ brew install elasticsearch"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "and"
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": "$ elasticsearch -f -D es.config=/usr/local/opt/elasticsearch/config/elasticsearch.yml"
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": "Follow"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "If you enjoyed this you should follow me on [Twitter](https://twitter.com/duu), [Github](https://github.com/pilt/), [Coderwall](https://coderwall.com/pilt), or [Geeklist](http://geekli.st/pilt)."
    }
   ],
   "metadata": {}
  }
 ]
}